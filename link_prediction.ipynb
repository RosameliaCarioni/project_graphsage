{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7c18JmeSuZW"
      },
      "source": [
        "https://medium.com/analytics-vidhya/ohmygraphs-graphsage-in-pyg-598b5ec77e7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O90RfNPbNn8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "import copy\n",
        "from tqdm import tqdm \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "from torch_geometric.transforms import RandomLinkSplit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.], device='mps:0')\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    x = torch.ones(1, device=device)\n",
        "    print (x)\n",
        "else:\n",
        "    print (\"MPS device not found.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data: CORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split data \n",
        "https://medium.com/stanford-cs224w/a-tour-of-pygs-data-loaders-9f2384e48f8f\n",
        "\n",
        "https://zqfang.github.io/2021-08-12-graph-linkpredict/ --> trasductie disjoint mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = RandomLinkSplit(\n",
        "    num_val=0.2,  # fraction of data held out for validation\n",
        "    num_test=0.2, # fraction of data held out for test\n",
        "    is_undirected= True, \n",
        "    add_negative_train_samples= True, \n",
        "    neg_sampling_ratio= 1.0, # Need to include negative sampling edges, the edges not existed in the original graph.\n",
        "    key = 'edge_label', \n",
        "    disjoint_train_ratio = 0.9\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlbwtdtM12Hk"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CVxVI6-R14nV"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001 # variable to change/play around with for experiments\n",
        "epochs = 10\n",
        "aggregator = 'mean' # variable to change/play around with for experiments\n",
        "dropout_rate = 0.4\n",
        "normalization = True\n",
        "activation_function = True\n",
        "bias = True\n",
        "batch =  512\n",
        "neighborhood_1 = 25\n",
        "neighborhood_2 = 10\n",
        "embedding_dimension = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeighborLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = LinkNeighborLoader(train_data, \n",
        "                            num_neighbors=[neighborhood_1, neighborhood_2],\n",
        "                            shuffle=True,\n",
        "                            neg_sampling_ratio= 1.0, \n",
        "                            batch_size = batch)\n",
        "\n",
        "# TODO: not sure if subgraph is needed\n",
        "subgraph_loader = LinkNeighborLoader(copy.copy(train_data), \n",
        "                                num_neighbors=[-1], \n",
        "                                shuffle=True,\n",
        "                               neg_sampling_ratio= 1.0, \n",
        "                                batch_size = batch )\n",
        "\n",
        "# No need to maintain these features during evaluation:\n",
        "del subgraph_loader.data.x, subgraph_loader.data.y\n",
        "# Add global node index information.\n",
        "subgraph_loader.data.num_nodes = data.num_nodes\n",
        "subgraph_loader.data.n_id = torch.arange(data.num_nodes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make model for Link prediction\n",
        "\n",
        "Initialization:\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html\n",
        "\n",
        "https://medium.com/@juyi.lin/neighborloader-introduction-ccb870cc7294\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphSAGE_local(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, aggr='mean', normalization = True, activation_function = True, bias = True):\n",
        "\n",
        "        super().__init__()\n",
        "        # as K = 2, we have 2 layers\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, out_channels = hidden_channels, project = activation_function, bias = bias)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels = out_channels, project = activation_function, bias = bias)\n",
        "        #self.conv3 = SAGEConv(128, out_channels = out_channels, normalize = normalization, project = activation_function, bias = bias)\n",
        "\n",
        "\n",
        "    def forward(self, matrix_nodes_features, edge_index):\n",
        "      # matrix_nodes_features is a matrix from the data where row = nodes, columns = feature\n",
        "      # edge_index: This is a tensor that describes the connectivity of the graph. Each column in this matrix represents an edge. The first row contains the indices of the source nodes, and the second row contains the indices of the target nodes.\n",
        "    \n",
        "        x = self.conv1(matrix_nodes_features, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        #x = self.conv3(x, edge_index)\n",
        "        #x = F.relu(x)\n",
        "        #x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        # returning only x allows to apply any task-specific post-processing (in this case: link prediction and node classification)\n",
        "        return x \n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def inference(self, x_all, subgraph_loader):\n",
        "        # designed to handle large graphs that don't fit into memory all at once. \n",
        "        # Instead of processing the entire graph at once, it processes the graph in smaller subgraphs or batches\n",
        "        \n",
        "        for i, conv in enumerate([self.conv1, self.conv2, self.conv3]):\n",
        "            xs = []\n",
        "            for batch in subgraph_loader:\n",
        "                x = x_all[batch.n_id.to(x_all.device)]\n",
        "                x = conv(x, batch.edge_index) \n",
        "                #x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
        "                #x = conv(x, batch.edge_index.to(device))\n",
        "                if i < len(self.convs) - 1:\n",
        "                    x = x.relu_()\n",
        "                xs.append(x[:batch.batch_size].cpu()) # we only need the representations of the target nodes\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "        return x_all"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rvsQcoTn3Zqd"
      },
      "source": [
        "# Create model for link prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VHTf6png0g8M"
      },
      "outputs": [],
      "source": [
        "labels_all_nodes = data.y\n",
        "number_classes  = labels_all_nodes.unique().size(0)\n",
        "\n",
        "model = GraphSAGE_local(in_channels = data.num_node_features,\n",
        "                  hidden_channels= data.num_node_features,\n",
        "                  out_channels = number_classes, # TODO: check results using 128 instead \n",
        "                  dropout= dropout_rate,\n",
        "                  aggr = aggregator,\n",
        "                  normalization = normalization,\n",
        "                  activation_function = activation_function,\n",
        "                  bias = bias)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm-wlKeK5VkW"
      },
      "source": [
        "# Train model with unsupervised loss \n",
        "Where model is trainned to fit training data \n",
        " The function iterates over batches of data from train_loader. Each batch contains a subset of the entire training dataset\n",
        " For each batch, the model computes the node embeddings h, then calculates the embeddings for the source h_src and destination h_dst nodes of each edge. It then predicts whether an edge should exist between node pairs (pred)\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train() # set model into training mode, doesnt do anything else \n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        #batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        h = model(batch.x, batch.edge_index)\n",
        "        h_src = h[batch.edge_label_index[0]]\n",
        "        h_dst = h[batch.edge_label_index[1]]\n",
        "        pred = (h_src * h_dst).sum(dim=-1)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.size(0)\n",
        "\n",
        "    return total_loss / data.num_nodes\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test method\n",
        "The test function evaluates the model's performance on unseen data. \n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(train_data.x, train_data.edge_index).cpu()\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    train_acc = clf.score(out, train_data.y) #TODO: fix this data \n",
        "    val_acc = clf.score(out, train_data.y) # TODO: fix this data \n",
        "\n",
        "    return train_acc, val_acc\n",
        "\n",
        "times = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index).cpu()\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    train_acc = clf.score(out[data.train_mask], data.y[data.train_mask]) #TODO: fix this data \n",
        "    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask]) # TODO: fix this data \n",
        "\n",
        "    return train_acc, val_acc\n",
        "\n",
        "times = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "n5MC9h_b5XQC",
        "outputId": "cca6d061-ef90-4707-a83c-e58cddec60c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 3.2272, Val: 0.1935, Train: 0.1935\n",
            "Epoch: 002, Loss: 3.2028, Val: 0.1370, Train: 0.1370\n",
            "Epoch: 003, Loss: 3.1907, Val: 0.1551, Train: 0.1551\n",
            "Epoch: 004, Loss: 3.1608, Val: 0.1702, Train: 0.1702\n",
            "Epoch: 005, Loss: 3.1326, Val: 0.2208, Train: 0.2208\n",
            "Epoch: 006, Loss: 3.0997, Val: 0.2153, Train: 0.2153\n",
            "Epoch: 007, Loss: 3.0658, Val: 0.2223, Train: 0.2223\n",
            "Epoch: 008, Loss: 3.0370, Val: 0.2057, Train: 0.2057\n",
            "Epoch: 009, Loss: 3.0359, Val: 0.2530, Train: 0.2530\n",
            "Median time per epoch: 2.0081s\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "for epoch in range(1, epochs):\n",
        "    start = time.time()\n",
        "    loss = train(epoch)\n",
        "    train_acc, val_acc = test()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Train: {train_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.edge_label "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data(x=[2423, 1433], edge_index=[2, 7907], y=[2423], train_mask=[2423], val_mask=[2423], test_mask=[2423], n_id=[2423], e_id=[7907], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get results for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "out = model(data.x, data.edge_index).to(device)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n",
        "print(val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JxEm6MvHzmAR",
        "DGzwcEMn5aWa"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
