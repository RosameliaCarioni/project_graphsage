{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7c18JmeSuZW"
      },
      "source": [
        "https://medium.com/analytics-vidhya/ohmygraphs-graphsage-in-pyg-598b5ec77e7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O90RfNPbNn8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "import copy\n",
        "from tqdm import tqdm \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "from torch_geometric.transforms import RandomLinkSplit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available() and False:\n",
        "    device = torch.device(\"mps\")\n",
        "    x = torch.ones(1, device=device)\n",
        "    print (x)\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data: CORA - small version "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Information from graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {data.x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# Print information about the graph\n",
        "print(f'\\nGraph:')\n",
        "print('------')\n",
        "print(f'Training nodes: {sum(data.train_mask).item()}')\n",
        "print(f'Evaluation nodes: {sum(data.val_mask).item()}')\n",
        "print(f'Test nodes: {sum(data.test_mask).item()}')\n",
        "print(f'Edges are directed: {data.is_directed()}')\n",
        "print(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Graph has loops: {data.has_self_loops()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.0001 # variable to change/play around with for experiments\n",
        "epochs = 10\n",
        "aggregator = 'mean' # variable to change/play around with for experiments\n",
        "dropout_rate = 0.4\n",
        "normalization = True\n",
        "activation_function = True\n",
        "bias = True\n",
        "batch =  512\n",
        "neighborhood_1 = 25\n",
        "neighborhood_2 = 10\n",
        "embedding_dimension = 128"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split data \n",
        "https://medium.com/stanford-cs224w/a-tour-of-pygs-data-loaders-9f2384e48f8f\n",
        "\n",
        "https://zqfang.github.io/2021-08-12-graph-linkpredict/ --> trasductie disjoint mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = RandomLinkSplit(\n",
        "    num_val=0.2,  # fraction of data held out for validation\n",
        "    num_test=0.2, # fraction of data held out for test\n",
        "    is_undirected= True, \n",
        "    add_negative_train_samples= True, \n",
        "    neg_sampling_ratio= 1.0, # Need to include negative sampling edges, the edges not existed in the original graph.\n",
        "    key = 'edge_label', \n",
        "    disjoint_train_ratio = 0.9\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)\n",
        "train_data, val_data, test_data =  train_data.to(device), val_data.to(device), test_data.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeighborLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = LinkNeighborLoader(train_data, \n",
        "                            num_neighbors=[neighborhood_1, neighborhood_2],\n",
        "                            shuffle=True,\n",
        "                            neg_sampling_ratio= 1.0, \n",
        "                            batch_size = batch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Information from sampling/subgraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print each subgraph\n",
        "for i, subgraph in enumerate(train_loader):\n",
        "    print(f'Subgraph {i}: {subgraph}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make model for Link prediction\n",
        "\n",
        "Initialization:\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html\n",
        "\n",
        "https://medium.com/@juyi.lin/neighborloader-introduction-ccb870cc7294\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphSAGE_local(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, aggr='mean', normalization = True, activation_function = True, bias = True):\n",
        "\n",
        "        super().__init__()\n",
        "        # as K = 2, we have 2 layers\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, out_channels = hidden_channels, project = activation_function, bias = bias)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels = out_channels, project = activation_function, bias = bias, normalize = normalization)\n",
        "        #self.conv3 = SAGEConv(128, out_channels = out_channels, normalize = normalization, project = activation_function, bias = bias)\n",
        "\n",
        "    def forward(self, matrix_nodes_features, edge_index):\n",
        "      # matrix_nodes_features is a matrix from the data where row = nodes, columns = feature\n",
        "      # edge_index: This is a tensor that describes the connectivity of the graph. Each column in this matrix represents an edge. The first row contains the indices of the source nodes, and the second row contains the indices of the target nodes.\n",
        "    \n",
        "        x = self.conv1(matrix_nodes_features, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training = self.training)\n",
        "\n",
        "        #x = self.conv3(x, edge_index)\n",
        "        #x = F.relu(x)\n",
        "        #x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        # returning only x allows to apply any task-specific post-processing (in this case: link prediction and node classification)\n",
        "        return x \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm-wlKeK5VkW"
      },
      "source": [
        "# Train model with unsupervised loss \n",
        "Where model is trainned to fit training data \n",
        " The function iterates over batches of data from train_loader. Each batch contains a subset of the entire training dataset\n",
        " \n",
        " For each batch, the model computes the node embeddings h, then calculates the embeddings for the source h_src and destination h_dst nodes of each edge. It then predicts whether an edge should exist between node pairs (pred)\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_loader, device):\n",
        "    model.train() # set model into training mode, doesnt do anything else \n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        h = model(batch.x, batch.edge_index)\n",
        "        h_src = h[batch.edge_label_index[0]]\n",
        "        h_dst = h[batch.edge_label_index[1]]\n",
        "        \n",
        "        pred = (h_src * h_dst).sum(dim=-1)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.size(0)\n",
        "\n",
        "    return total_loss / len(train_loader)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test method\n",
        "The test function evaluates the model's performance on unseen data. \n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    \n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    train_acc = clf.score(out[data.train_mask], data.y[data.train_mask]) \n",
        "    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask]) \n",
        "    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n",
        "    return train_acc, val_acc, test_acc\n",
        "\n",
        "# TODO: this test maybe should return other things "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create GraphSAGE model for link prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphSAGE_local(\n",
            "  (conv1): SAGEConv(1433, 1433, aggr=mean)\n",
            "  (conv2): SAGEConv(1433, 128, aggr=mean)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = GraphSAGE_local(in_channels = data.num_node_features,\n",
        "                  hidden_channels= data.num_node_features,\n",
        "                  out_channels = embedding_dimension, # TODO: check results using 128 instead \n",
        "                  dropout= dropout_rate,\n",
        "                  aggr = aggregator,\n",
        "                  normalization = normalization,\n",
        "                  activation_function = activation_function,\n",
        "                  bias = bias)\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training model for certain number for epochs and testing it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 000, Loss: 440.5940, Val: 0.3180, Train: 0.7357, Test: 0.3190\n",
            "Epoch: 001, Loss: 408.4323, Val: 0.3300, Train: 0.7786, Test: 0.3260\n",
            "Epoch: 002, Loss: 387.4183, Val: 0.3500, Train: 0.8214, Test: 0.3320\n",
            "Epoch: 003, Loss: 372.9583, Val: 0.3440, Train: 0.8429, Test: 0.3400\n",
            "Epoch: 004, Loss: 358.2499, Val: 0.3520, Train: 0.8357, Test: 0.3560\n",
            "Epoch: 005, Loss: 352.2124, Val: 0.3740, Train: 0.8500, Test: 0.3640\n",
            "Epoch: 006, Loss: 343.6900, Val: 0.3760, Train: 0.8643, Test: 0.3590\n",
            "Epoch: 007, Loss: 339.8025, Val: 0.3780, Train: 0.8643, Test: 0.3570\n",
            "Epoch: 008, Loss: 336.5369, Val: 0.3860, Train: 0.8571, Test: 0.3580\n",
            "Epoch: 009, Loss: 337.3190, Val: 0.3880, Train: 0.8500, Test: 0.3550\n",
            "Epoch: 010, Loss: 338.2898, Val: 0.4100, Train: 0.8500, Test: 0.3610\n",
            "Median time per epoch: 0.3126s\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs+1):\n",
        "    start = time.time()\n",
        "    loss = train(model, optimizer, train_loader, device)\n",
        "    train_acc, val_acc, test_acc = test(model, data)\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Train: {train_acc:.4f}, Test: {test_acc:.4f}')\n",
        "    \n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data(x=[2423, 1433], edge_index=[2, 7907], y=[2423], train_mask=[2423], val_mask=[2423], test_mask=[2423], n_id=[2423], e_id=[7907], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get results for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 5\u001b[0m clf\u001b[39m.\u001b[39;49mfit(out[data\u001b[39m.\u001b[39;49mtrain_mask], data\u001b[39m.\u001b[39;49my[data\u001b[39m.\u001b[39;49mtrain_mask])\n\u001b[1;32m      6\u001b[0m val_acc \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mscore(out[data\u001b[39m.\u001b[39mval_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mval_mask])\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(val_acc)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1208\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1206\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1208\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1209\u001b[0m     X,\n\u001b[1;32m   1210\u001b[0m     y,\n\u001b[1;32m   1211\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1212\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1213\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1214\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1215\u001b[0m )\n\u001b[1;32m   1216\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/utils/validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[0;32m-> 1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1149\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1150\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1151\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1152\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1153\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1154\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1155\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1156\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1157\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1158\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1164\u001b[0m check_consistent_length(X, y)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/utils/validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    914\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 915\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    916\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    918\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    919\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/torch/_tensor.py:1032\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m   1031\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "out = model(data.x, data.edge_index).to(device)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n",
        "print(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JxEm6MvHzmAR",
        "DGzwcEMn5aWa"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
