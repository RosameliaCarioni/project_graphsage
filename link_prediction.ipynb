{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7c18JmeSuZW"
      },
      "source": [
        "https://medium.com/analytics-vidhya/ohmygraphs-graphsage-in-pyg-598b5ec77e7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O90RfNPbNn8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "import copy\n",
        "from tqdm import tqdm \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "from torch_geometric.transforms import RandomLinkSplit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.], device='mps:0')\n"
          ]
        }
      ],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    x = torch.ones(1, device=device)\n",
        "    print (x)\n",
        "else:\n",
        "    print (\"MPS device not found.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data: CORA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "data = dataset[0]\n",
        "data = data.to(device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split data \n",
        "https://medium.com/stanford-cs224w/a-tour-of-pygs-data-loaders-9f2384e48f8f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = RandomLinkSplit(\n",
        "    num_val=0.2,  # fraction of data held out for validation\n",
        "    num_test=0.2, # fraction of data held out for test\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlbwtdtM12Hk"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CVxVI6-R14nV"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001 # variable to change/play around with for experiments\n",
        "epochs = 10\n",
        "aggregator = 'mean' # variable to change/play around with for experiments\n",
        "dropout_rate = 0.4\n",
        "normalization = True\n",
        "activation_function = True\n",
        "bias = True\n",
        "batch =  512\n",
        "neighborhood_1 = 25\n",
        "neighborhood_2 = 10\n",
        "embedding_dimension = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NeighborLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/torch_geometric/utils/sparse.py:484: UserWarning: The operator 'aten::_convert_indices_from_coo_to_csr.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
            "  return torch._convert_indices_from_coo_to_csr(\n"
          ]
        }
      ],
      "source": [
        "train_loader = LinkNeighborLoader(train_data, \n",
        "                            num_neighbors=[neighborhood_1, neighborhood_2],\n",
        "                            shuffle=True)\n",
        "\n",
        "# TODO: not sure if subgraph is needed\n",
        "subgraph_loader = LinkNeighborLoader(copy.copy(train_data), \n",
        "                                num_neighbors=[-1], \n",
        "                                shuffle=True)\n",
        "\n",
        "# No need to maintain these features during evaluation:\n",
        "del subgraph_loader.data.x, subgraph_loader.data.y\n",
        "# Add global node index information.\n",
        "subgraph_loader.data.num_nodes = data.num_nodes\n",
        "subgraph_loader.data.n_id = torch.arange(data.num_nodes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make model for Link prediction\n",
        "\n",
        "Initialization:\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html\n",
        "\n",
        "https://medium.com/@juyi.lin/neighborloader-introduction-ccb870cc7294\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphSAGE_local(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, aggr='mean', normalization = True, activation_function = True, bias = True):\n",
        "      # TODO: maybe activation_function should be False\n",
        "\n",
        "        super().__init__()\n",
        "        # as K = 2, we have 2 layers\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(in_channels, out_channels = hidden_channels, project = activation_function, bias = bias)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels = out_channels, project = activation_function, bias = bias)\n",
        "        #self.conv3 = SAGEConv(128, out_channels = out_channels, normalize = normalization, project = activation_function, bias = bias)\n",
        "\n",
        "\n",
        "    def forward(self, matrix_nodes_features, edge_index):\n",
        "      # matrix_nodes_features is a matrix from the data where row = nodes, columns = feature\n",
        "      # edge_index: This is a tensor that describes the connectivity of the graph. Each column in this matrix represents an edge. The first row contains the indices of the source nodes, and the second row contains the indices of the target nodes.\n",
        "    \n",
        "        x = self.conv1(matrix_nodes_features, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        #x = self.conv3(x, edge_index)\n",
        "        #x = F.relu(x)\n",
        "        #x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        # returning only x allows to apply any task-specific post-processing (in this case: link prediction and node classification)\n",
        "        return x \n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def inference(self, x_all, subgraph_loader):\n",
        "        # designed to handle large graphs that don't fit into memory all at once. \n",
        "        # Instead of processing the entire graph at once, it processes the graph in smaller subgraphs or batches\n",
        "        \n",
        "        for i, conv in enumerate([self.conv1, self.conv2, self.conv3]):\n",
        "            xs = []\n",
        "            for batch in subgraph_loader:\n",
        "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
        "                x = conv(x, batch.edge_index.to(device))\n",
        "                if i < len(self.convs) - 1:\n",
        "                    x = x.relu_()\n",
        "                xs.append(x[:batch.batch_size].cpu()) # we only need the representations of the target nodes\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "        return x_all"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rvsQcoTn3Zqd"
      },
      "source": [
        "# Create model for link prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VHTf6png0g8M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GraphSAGE_local(\n",
              "  (conv1): SAGEConv(1433, 1433, aggr=mean)\n",
              "  (conv2): SAGEConv(1433, 7, aggr=mean)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_all_nodes = data.y\n",
        "number_classes  = labels_all_nodes.unique().size(0)\n",
        "\n",
        "model = GraphSAGE_local(in_channels = data.num_node_features,\n",
        "                  hidden_channels= data.num_node_features,\n",
        "                  out_channels = number_classes, # TODO: check results using 128 instead \n",
        "                  dropout= dropout_rate,\n",
        "                  aggr = aggregator,\n",
        "                  normalization = normalization,\n",
        "                  activation_function = activation_function,\n",
        "                  bias = bias)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm-wlKeK5VkW"
      },
      "source": [
        "# Train model with unsupervised loss \n",
        "Where model is trainned to fit training data \n",
        " The function iterates over batches of data from train_loader. Each batch contains a subset of the entire training dataset\n",
        " For each batch, the model computes the node embeddings h, then calculates the embeddings for the source h_src and destination h_dst nodes of each edge. It then predicts whether an edge should exist between node pairs (pred)\n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train() # set model into training mode, doesnt do anything else \n",
        "\n",
        "    #printing bars of progress\n",
        "    pbar = tqdm(total=int(len(train_loader.dataset)))\n",
        "    pbar.set_description(f'Epoch {epoch:02d}')\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        print('hey ollie')\n",
        "        print(batch.edge_label_index)\n",
        "        \n",
        "        h = model(batch.x, batch.edge_index)\n",
        "        h_src = h[batch.edge_label_index[0]]\n",
        "        h_dst = h[batch.edge_label_index[1]]\n",
        "        pred = (h_src * h_dst).sum(dim=-1)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.size(0)\n",
        "        pbar.update(batch.batch_size)\n",
        "\n",
        "    pbar.close()\n",
        "    return total_loss / data.num_nodes\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test method\n",
        "The test function evaluates the model's performance on unseen data. \n",
        "\n",
        "https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index).to(device)\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    train_acc = clf.score(out[data.train_mask], data.y[data.train_mask])\n",
        "    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "    return train_acc, val_acc\n",
        "\n",
        "times = []"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "n5MC9h_b5XQC",
        "outputId": "cca6d061-ef90-4707-a83c-e58cddec60c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Epoch 01:   0%|          | 0/6334 [03:22<?, ?it/s]\n",
            "Epoch 01:   0%|          | 0/6334 [00:16<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hey ollie\n",
            "tensor([[1],\n",
            "        [0]], device='mps:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'GlobalStorage' object has no attribute 'edge_label'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m51\u001b[39m):\n\u001b[1;32m      3\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[39m=\u001b[39m train(epoch)\n\u001b[1;32m      5\u001b[0m     train_acc, val_acc \u001b[39m=\u001b[39m test()\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      7\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVal: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[26], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m h_dst \u001b[39m=\u001b[39m h[batch\u001b[39m.\u001b[39medge_label_index[\u001b[39m1\u001b[39m]]\n\u001b[1;32m     19\u001b[0m pred \u001b[39m=\u001b[39m (h_src \u001b[39m*\u001b[39m h_dst)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(pred, batch\u001b[39m.\u001b[39;49medge_label)\n\u001b[1;32m     21\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/torch_geometric/data/data.py:482\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    477\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 482\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
            "File \u001b[0;32m~/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/torch_geometric/data/storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'edge_label'"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "for epoch in range(1, 51):\n",
        "    start = time.time()\n",
        "    loss = train(epoch)\n",
        "    train_acc, val_acc = test()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "          f'Val: {val_acc:.4f}, Train: {train_acc:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data(x=[2423, 1433], edge_index=[2, 7907], y=[2423], train_mask=[2423], val_mask=[2423], test_mask=[2423], n_id=[2423], e_id=[7907], input_id=[256], edge_label_index=[2, 512], edge_label=[512])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get results for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "out = model(data.x, data.edge_index).to(device)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n",
        "print(val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JxEm6MvHzmAR",
        "DGzwcEMn5aWa"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
