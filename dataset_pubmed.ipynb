{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphsage_calculate_embeddings\n",
    "import test_embeddings\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Could not resolve 'map' among choices {'MLPAggregation', 'StdAggregation', 'DeepSetsAggregation', 'EquilibriumAggregation', 'VarAggregation', 'AttentionalAggregation', 'SumAggregation', 'MinAggregation', 'GRUAggregation', 'MaxAggregation', 'MedianAggregation', 'GraphMultisetTransformer', 'Set2Set', 'SoftmaxAggregation', 'DegreeScalerAggregation', 'SetTransformerAggregation', 'MeanAggregation', 'LCMAggregation', 'QuantileAggregation', 'PowerMeanAggregation', 'MultiAggregation', 'Aggregation', 'LSTMAggregation', 'SortAggregation', 'add', 'MulAggregation'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001] # variable to change/play around with for experiments --> 0.0001\n",
    "aggregators = ['MeanAggregation', 'MaxAggregation', 'LSTMAggregation'] # variable to change/play around with for experiments\n",
    "projects = [True, False]\n",
    "directed_graph = True\n",
    "\n",
    "# FIXED PARAMS \n",
    "epochs = 10\n",
    "dropout_rate = 0.4\n",
    "normalization = True \n",
    "activation_function = F.relu\n",
    "bias = True\n",
    "batch_size =  512\n",
    "neighborhood_1 = 25\n",
    "neighborhood_2 = 10\n",
    "embedding_dimension = 128\n",
    "hidden_layer = 512\n",
    "#project = True # layer applies a linear transformation followed by an activation function before aggreagation, as described in EQ. 3 of paper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features, number_nodes = data.num_features, data.x.shape[0]\n",
    "data = data.sort(sort_by_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "broken_experiments = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for aggregator in aggregators:\n",
    "        for project in projects:\n",
    "            try: \n",
    "                start_time = time.time()\n",
    "                # Compute the embedding matrix for the current set of hyperparameters\n",
    "                embedding_matrix = graphsage_calculate_embeddings.compute_embedding_matrix(\n",
    "                    data=data,\n",
    "                    number_features=number_features,\n",
    "                    number_nodes=number_nodes,\n",
    "                    batch_size=batch_size,\n",
    "                    hidden_layer=hidden_layer,\n",
    "                    epochs=epochs,\n",
    "                    neighborhood_1=neighborhood_1,\n",
    "                    neighborhood_2=neighborhood_2,\n",
    "                    embedding_dimension=embedding_dimension,\n",
    "                    learning_rate=lr,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    activation_function=activation_function,\n",
    "                    aggregator=aggregator,\n",
    "                    activation_before_normalization=True, \n",
    "                    bias=True,\n",
    "                    normalize=normalization, \n",
    "                    project=project\n",
    "                )\n",
    "                \n",
    "                # Store the embedding matrix and corresponding hyperparameters\n",
    "                results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'aggregator': aggregator,\n",
    "                    'embedding_matrix': embedding_matrix, \n",
    "                    'time': time.time() - start_time\n",
    "                })\n",
    "                torch.save(embedding_matrix, f\"embeddings/pubmed/{lr}_{aggregator}_{project}_.pt\")\n",
    "            except Exception as e:\n",
    "                broken_experiments.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'aggregator': aggregator,\n",
    "                    'embedding_matrix': embedding_matrix, \n",
    "                    'time': time.time() - start_time,\n",
    "                    'error': traceback.format_exc()\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding_matrix, 'embeddings/cora_small.pt') #TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate node classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1_macro, f1_micro = test_embeddings.test_node_classification_one_class(embedding_matrix, data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {acc*100:.4f}, F1_macro: {f1_macro*100:.4f}, F1_micro: {f1_micro*100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform embedding matrix into numpy \n",
    "embedding_detached = embedding_matrix.detach()\n",
    "embedding_np = embedding_detached.numpy()\n",
    "\n",
    "# Obtain edges and non existing edges as lists \n",
    "edges, non_edges = test_embeddings.get_edges_and_non_edges_as_lists(data, directed_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = test_embeddings.test_link_prediction_k_fold_validation(embedding_matrix, edges, non_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC Score:\", roc_auc*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_sage_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
