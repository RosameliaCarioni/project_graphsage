{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import graphsage_calculate_embeddings\n",
    "import torch.nn.functional as F\n",
    "import read_data\n",
    "import test_embeddings\n",
    "import locale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Dataset\n",
    "\n",
    "nodes.csv: Contains the node IDs, where each ID represents a unique user on YouTube.\n",
    "\n",
    "groups.csv:  Contains group IDs, each representing a different group or community within the YouTube platform.\n",
    "\n",
    "edges.csv:  Comprises pairs of user IDs, with each pair indicating a friendship link between two users. For example, an entry \"1,2\" means that the user with ID \"1\" is friends with the user with ID \"2\".\n",
    "\n",
    "group-edges.csv: Each line has two numbers; the first is a user ID, and the second is a group ID. This file links users to the groups they are part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "nodes_path = 'datasets/YouTube-dataset/data/nodes.csv'\n",
    "edges_path = 'datasets/YouTube-dataset/data/edges.csv'\n",
    "groups_path = 'datasets/YouTube-dataset/data/groups.csv'\n",
    "group_edges_path = 'datasets/Youtube-dataset/data/group-edges.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graph object and transform it to torch_geometric.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(nodes_path, edges_path, groups_path, group_edges_path):\n",
    "    \"\"\"Read in the data from the csv files and transform it into networkx object. \n",
    "    If a node has no labels, this is removed from the outputed/created graph.  \n",
    "\n",
    "    Args:\n",
    "        nodes_path: path to the csv file \n",
    "        edges_path: path to the csv file \n",
    "        groups_path: path to the csv file \n",
    "        group_edges_path: path to the csv \n",
    "            \n",
    "    Returns:\n",
    "        graph, labels or classes to the nodes \n",
    "    \"\"\"\n",
    "    nodes_id = pd.read_csv(nodes_path, header=None, names=['id'])\n",
    "    #groups_id = pd.read_csv(groups_path, header=None, names=['group'])\n",
    "    edges = pd.read_csv(edges_path, header=None, names=['id_1', 'id_2'])\n",
    "    user_group_membership = pd.read_csv(group_edges_path, header=None, names=['id', 'group'])\n",
    "\n",
    "    # Sort the node pairs and drop duplicates to ensure each edge is unique\n",
    "    edges[['id_1', 'id_2']] = np.sort(edges[['id_1', 'id_2']], axis=1)\n",
    "    edges = edges.drop_duplicates()\n",
    "\n",
    "    # Create a graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    #G_BC.add_nodes_from(nodes_id['id'])\n",
    "    for node_id in nodes_id['id']:\n",
    "        graph.add_node(node_id, id=node_id)\n",
    "    \n",
    "    # Add edges (friendships) to the graph\n",
    "    graph.add_edges_from(edges[['id_1', 'id_2']].values)\n",
    "\n",
    "    # Create a dictionary to store groups for each ID\n",
    "    group_dict = {}\n",
    "\n",
    "    # Populate the group_dict\n",
    "    for _, row in user_group_membership.iterrows():\n",
    "        user_id = row['id']\n",
    "        group_id = row['group']\n",
    "\n",
    "        # Check if the user_id is already in the dictionary\n",
    "        if user_id in group_dict:\n",
    "            group_dict[user_id].append(group_id)\n",
    "        else:\n",
    "            group_dict[user_id] = [group_id]\n",
    "\n",
    "    # Add group labels to the nodes\n",
    "    for user_id, groups in group_dict.items():\n",
    "        nx.set_node_attributes(graph, {user_id: groups}, 'y') # 'group belonging'\n",
    "    \n",
    "    # Remove nodes without labels\n",
    "    for node_id in list(graph.nodes):\n",
    "        groups = group_dict.get(node_id)\n",
    "        if not groups:\n",
    "            graph.remove_node(node_id)\n",
    "        else:\n",
    "            nx.set_node_attributes(graph, {node_id: groups}, 'y')\n",
    "\n",
    "    # Find and preprocess labels for the graph\n",
    "    labels = []\n",
    "    c = 0 \n",
    "    for n in graph.nodes:\n",
    "        l = graph.nodes[n].get('y')\n",
    "        labels.append(l)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    preprocessed_labels = mlb.fit_transform(labels)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    preprocessed_labels = mlb.fit_transform(labels)\n",
    "\n",
    "    return graph, preprocessed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, labels = read_dataset(nodes_path, edges_path, groups_path, group_edges_path)\n",
    "data = from_networkx(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting labels and features \n",
    "data.y = torch.from_numpy(labels.astype(float))\n",
    "# Add x variable which diagonal matrix with 1's as entries and size = num_nodes x num_nodes\n",
    "x_diagonal = torch.eye(data.num_nodes)\n",
    "data.x = x_diagonal.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001 \n",
    "aggregator = 'MeanAggregation'\n",
    "\n",
    "epochs = 10\n",
    "dropout_rate = 0.4\n",
    "normalization = True \n",
    "activation_function = F.relu\n",
    "bias = True\n",
    "batch_size =  512\n",
    "neighborhood_1 = 25\n",
    "neighborhood_2 = 10\n",
    "embedding_dimension = 128\n",
    "hidden_layer = 512\n",
    "project = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features, number_nodes = data.num_features, data.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31703"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 192722], y=[31703, 47], id=[31703], num_nodes=31703, x=[31703, 31703])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 1/11 [33:54<5:39:03, 2034.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Total loss: 3.3590, time_taken: 2034.3418009281158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 2/11 [1:10:24<5:18:54, 2126.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Total loss: 3.2139, time_taken: 2190.173082113266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  27%|██▋       | 3/11 [1:47:17<4:48:45, 2165.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Total loss: 3.1632, time_taken: 2212.9766280651093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  36%|███▋      | 4/11 [2:20:55<4:05:51, 2107.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Total loss: 3.1482, time_taken: 2018.0388810634613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  45%|████▌     | 5/11 [2:54:30<3:27:23, 2073.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Total loss: 3.1451, time_taken: 2014.6921288967133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  55%|█████▍    | 6/11 [3:28:03<2:51:07, 2053.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Total loss: 3.1372, time_taken: 2013.6688272953033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  64%|██████▎   | 7/11 [4:01:34<2:15:57, 2039.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Total loss: 3.1391, time_taken: 2010.5166671276093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  73%|███████▎  | 8/11 [4:35:05<1:41:31, 2030.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Total loss: 3.1352, time_taken: 2011.4325149059296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  82%|████████▏ | 9/11 [5:08:37<1:07:29, 2024.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Total loss: 3.1324, time_taken: 2011.5233118534088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  91%|█████████ | 10/11 [5:42:04<33:39, 2019.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Total loss: 3.1347, time_taken: 2007.328714132309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 11/11 [6:15:37<00:00, 2048.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Total loss: 3.1301, time_taken: 2012.5234701633453\n",
      "Median time per epoch: 2013.6693s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = graphsage_calculate_embeddings.compute_embedding_matrix(\n",
    "    data=data,\n",
    "    number_features=number_features,\n",
    "    number_nodes=number_nodes,\n",
    "    batch_size=batch_size,\n",
    "    hidden_layer=hidden_layer,\n",
    "    epochs=epochs,\n",
    "    neighborhood_1=neighborhood_1,\n",
    "    neighborhood_2=neighborhood_2,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    activation_function=activation_function,\n",
    "    aggregator=aggregator,\n",
    "    activation_before_normalization=True, \n",
    "    bias=True,\n",
    "    normalize=normalization, \n",
    "    project=project\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'embeddings/youtube.pt'\n",
    "torch.save(embedding_matrix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load it again: \n",
    "embedding_matrix = torch.load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/rosameliacarioni/miniconda3/envs/graph_sage_6/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "acc, f1_macro, f1_micro = test_embeddings.test_node_classification_multi_class(embedding_matrix, data.y)\n",
    "#print(f\"Accuracy: {acc*100:.4f}, F1_macro: {f1_macro*100:.4f}, F1_micro: {f1_micro*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, 'de_DE')\n",
    "\n",
    "# Format the numbers with four digits after the decimal and replace the dot with a comma\n",
    "formatted_acc = locale.format_string(\"%.4f\", acc * 100).replace('.', ',')\n",
    "formatted_f1_macro = locale.format_string(\"%.4f\", f1_macro * 100).replace('.', ',')\n",
    "formatted_f1_micro = locale.format_string(\"%.4f\", f1_micro * 100).replace('.', ',')\n",
    "\n",
    "print(f\"Accuracy: {formatted_acc}, F1_macro: {formatted_f1_macro}, F1_micro: {formatted_f1_micro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = test_embeddings.train_test_split_graph(data = data, is_undirected = True) # TODO: change the is_undirected depending on graph\n",
    "\n",
    "# Prepare edges\n",
    "test_edges = test_data.edge_label_index.numpy().T\n",
    "y_true = test_data.edge_label.numpy()\n",
    "\n",
    "# Prepare embeddings\n",
    "embedding_detached = embedding_matrix.detach()\n",
    "embedding_np = embedding_detached.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score = test_embeddings.k_fold_cross_validation_link_prediction(embedding_np, test_edges, y_true, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_score = \"{:.4f}\".format(roc_auc_score * 100).replace('.', ',')\n",
    "print(\"ROC AUC Score:\", formatted_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_sage_6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
